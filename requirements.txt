Use any programming language or framework of your choice for model training.
- Jupyter Notebook (python)

Preprocess the data and split the dataset into training and testing sets (e.g., 80% training, 20% testing).
- "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"

Train a classification model on the training set to predict the applicationStatus label.
- Convert applicationStatus to numerical value using Label Encoder to see the relationships between the dataset(0=Rejected, 1=Approved).
- Use correlation matrix to see the relationships of applicationStatus label ('financeAmount', 'loanTenure').
*If the value close or near to 1, meaning the the relationships effect the causes of loan approval.
- The training process shows that the model is learning from the training data, 
as evidenced by the decreasing loss and increasing accuracy over epochs.
"model.fit(X_train_scaled, y_train, epochs=15, batch_size=82, verbose=1)"

Evaluate the model on the testing set and report the performance metrics (e.g., accuracy, precision, recall, F1-score).
- The accuracy of 0.805 indicates that approximately 80.5% of the samples in the testing set were correctly classified by the model.
- 80.5% precision the model predicts an application as approved.
- 1.0 recall the model correctly identifies all positive instances (approved applications) in the testing set.
- 0.892 F1 suggests that the model achieves a good balance between precision and recall.

Provide any necessary data preprocessing steps and explain your choice of model.
- Checking the data if there is missing value or NaN values in dataset.
- Use imputation method to replace the missing value in dataset.
("imputer = SimpleImputer(strategy='mean')")
- Use Keras Model because the model consists of three layers: 
	- Input layer: The input layer has two neurons, corresponding to the features 'financeAmount' and 'loanTenure'. 
	- Hidden layers: Two hidden layers with 64 and 32 neurons respectively, both using the ReLU activation function. ReLU is a commonly used activation function that introduces non-linearity into the model, enabling it to learn complex mappings. 
	- Output layer: The output layer has one neuron with a sigmoid activation function, which outputs a probability between 0 and 1 representing the likelihood of loan approval.